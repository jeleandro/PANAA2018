{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para o PAN - Atribuição Autoral - 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#python basic libs\n",
    "from __future__ import print_function\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "import os;\n",
    "from os.path import join as pathjoin;\n",
    "\n",
    "import re;\n",
    "import glob;\n",
    "import json;\n",
    "import codecs;\n",
    "from collections import defaultdict;\n",
    "import pprint;\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "\n",
    "#data analysis libs\n",
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt;\n",
    "import random;\n",
    "\n",
    "#machine learning libs\n",
    "#feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#preprocessing and transformation\n",
    "from sklearn.preprocessing import normalize, MaxAbsScaler, MinMaxScaler;\n",
    "from sklearn.preprocessing import LabelBinarizer;\n",
    "from sklearn.decomposition import PCA;\n",
    "from sklearn.metrics.pairwise import cosine_similarity;\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "#classifiers\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFE,SelectFpr,SelectPercentile, chi2;\n",
    "\n",
    "#\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#model valuation\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "sns.set(color_codes=True);\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin-17.5.0-x86_64-i386-64bit\n",
      "NumPy 1.14.2\n",
      "SciPy 1.0.1\n",
      "Scikit-Learn 0.19.1\n",
      "seaborn 0.8.1\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.platform())\n",
    "print(\"NumPy\", np.__version__)\n",
    "import scipy; print(\"SciPy\", scipy.__version__)\n",
    "import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\n",
    "print(\"seaborn\", sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paths configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = '/Users/joseeleandrocustodio/Dropbox/mestrado/02 - Pesquisa/code';\n",
    "\n",
    "inputDir= pathjoin(baseDir,'pan18aa');\n",
    "outputDir= pathjoin(baseDir,'out',\"oficial\");\n",
    "if not os.path.exists(outputDir):\n",
    "    os.mkdir(outputDir);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCollectionsOfProblems(path):\n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    with open(infocollection, 'r') as f:\n",
    "        problems  = [\n",
    "            {\n",
    "                'problem': attrib['problem-name'],\n",
    "                'language': attrib['language'],\n",
    "                'encoding': attrib['encoding'],\n",
    "            }\n",
    "            for attrib in json.load(f)\n",
    "            \n",
    "        ]\n",
    "    return problems;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = readCollectionsOfProblems(inputDir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': u'UTF-8', 'language': u'en', 'problem': u'problem00001'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readProblem(path, problem):\n",
    "    # Reading information about the problem\n",
    "    infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "    candidates = []\n",
    "    with open(infoproblem, 'r') as f:\n",
    "        fj = json.load(f)\n",
    "        unk_folder = fj['unknown-folder']\n",
    "        for attrib in fj['candidate-authors']:\n",
    "            candidates.append(attrib['author-name'])\n",
    "    return unk_folder, candidates;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path,label):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(pathjoin(path,label,'*.txt'))\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label, os.path.basename(v)))\n",
    "        f.close()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index,problem in enumerate(problems):\n",
    "    unk_folder, candidates_folder = readProblem(inputDir, problem['problem']); \n",
    "    problem['candidates_folder_count'] = len(candidates_folder);\n",
    "    problem['candidates'] = [];\n",
    "    for candidate in candidates_folder:\n",
    "        problem['candidates'].extend(read_files(pathjoin(inputDir, problem['problem']),candidate));\n",
    "    \n",
    "    problem['unknown'] = read_files(pathjoin(inputDir, problem['problem']),unk_folder);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>candidates_folder_count</th>\n",
       "      <th>encoding</th>\n",
       "      <th>language</th>\n",
       "      <th>problem</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(graceful ones.\\n\\n\"One more,\" Marvelous said...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>[(after all, his best friends. And what in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(a mission.\"\\n\\nJensen just raises an eyebrow...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>[(“Potter was attractive,” Draco thought, sigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(qui l'avait tué mais tout était de la faute ...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>problem00003</td>\n",
       "      <td>[(son réveil. Sa main pulse et Draco frotte l'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(. Le canapé est vide et lorsqu'il passe deva...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>problem00004</td>\n",
       "      <td>[(abasourdie.\\n\\nTout d'abord, elle crut que s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(Eppure lui la mappa l’aveva stampata, dannaz...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>problem00005</td>\n",
       "      <td>[(– Oh. Cazzo.\\nSirius era così sconvolto che ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(Yato ha trovato una lettera sul suo comodino...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>problem00006</td>\n",
       "      <td>[(così la tua vista, Moony?\\n– Cercavo di esse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[(zmienił zdanie. Niech się stworzonko pobawi....</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>problem00007</td>\n",
       "      <td>[(dawniej pełna radości i ciepła, a teraz wiec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[(Słowem, które Sherlock najczęściej słyszał w...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>problem00008</td>\n",
       "      <td>[(, uderzającego o żebra niczym dzwon- niemal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[(pero no lo ama como ama a Guignol –explicó e...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>problem00009</td>\n",
       "      <td>[(–La nariz puntiaguda del elfo casi rozaba el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[(incapaz de señalar un momento exacto, un pun...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>problem00010</td>\n",
       "      <td>[(tan parecidas hizo que su trasero latiese de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          candidates  candidates_folder_count  \\\n",
       "0  [(graceful ones.\\n\\n\"One more,\" Marvelous said...                       20   \n",
       "1  [(a mission.\"\\n\\nJensen just raises an eyebrow...                        5   \n",
       "2  [(qui l'avait tué mais tout était de la faute ...                       20   \n",
       "3  [(. Le canapé est vide et lorsqu'il passe deva...                        5   \n",
       "4  [(Eppure lui la mappa l’aveva stampata, dannaz...                       20   \n",
       "5  [(Yato ha trovato una lettera sul suo comodino...                        5   \n",
       "6  [(zmienił zdanie. Niech się stworzonko pobawi....                       20   \n",
       "7  [(Słowem, które Sherlock najczęściej słyszał w...                        5   \n",
       "8  [(pero no lo ama como ama a Guignol –explicó e...                       20   \n",
       "9  [(incapaz de señalar un momento exacto, un pun...                        5   \n",
       "\n",
       "  encoding language       problem  \\\n",
       "0    UTF-8       en  problem00001   \n",
       "1    UTF-8       en  problem00002   \n",
       "2    UTF-8       fr  problem00003   \n",
       "3    UTF-8       fr  problem00004   \n",
       "4    UTF-8       it  problem00005   \n",
       "5    UTF-8       it  problem00006   \n",
       "6    UTF-8       pl  problem00007   \n",
       "7    UTF-8       pl  problem00008   \n",
       "8    UTF-8       sp  problem00009   \n",
       "9    UTF-8       sp  problem00010   \n",
       "\n",
       "                                             unknown  \n",
       "0  [(after all, his best friends. And what in the...  \n",
       "1  [(“Potter was attractive,” Draco thought, sigh...  \n",
       "2  [(son réveil. Sa main pulse et Draco frotte l'...  \n",
       "3  [(abasourdie.\\n\\nTout d'abord, elle crut que s...  \n",
       "4  [(– Oh. Cazzo.\\nSirius era così sconvolto che ...  \n",
       "5  [(così la tua vista, Moony?\\n– Cercavo di esse...  \n",
       "6  [(dawniej pełna radości i ciepła, a teraz wiec...  \n",
       "7  [(, uderzającego o żebra niczym dzwon- niemal ...  \n",
       "8  [(–La nariz puntiaguda del elfo casi rozaba el...  \n",
       "9  [(tan parecidas hizo que su trasero latiese de...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******************************************************************************************************\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def eval_measures(gt, pred):\n",
    "    \"\"\"Compute macro-averaged F1-scores, macro-averaged precision, \n",
    "    macro-averaged recall, and micro-averaged accuracy according the ad hoc\n",
    "    rules discussed at the top of this file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt : dict\n",
    "        Ground truth, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    pred : dict\n",
    "        Predicted attribution, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Macro-averaged F1-score\n",
    "    precision : float\n",
    "        Macro-averaged precision\n",
    "    recall : float\n",
    "        Macro-averaged recall\n",
    "    accuracy : float\n",
    "        Micro-averaged F1-score\n",
    "    \"\"\"\n",
    "\n",
    "    actual_authors = list(gt.values())\n",
    "    encoder = LabelEncoder().fit(['<UNK>'] + actual_authors)\n",
    "\n",
    "    text_ids, gold_authors, silver_authors = [], [], []\n",
    "    for text_id in sorted(gt):\n",
    "        text_ids.append(text_id)\n",
    "        gold_authors.append(gt[text_id])\n",
    "        try:\n",
    "            silver_authors.append(pred[text_id])\n",
    "        except KeyError:\n",
    "            # missing attributions get <UNK>:\n",
    "            silver_authors.append('<UNK>')\n",
    "\n",
    "    assert len(text_ids) == len(gold_authors)\n",
    "    assert len(text_ids) == len(silver_authors)\n",
    "\n",
    "    # replace non-existent silver authors with '<UNK>':\n",
    "    silver_authors = [a if a in encoder.classes_ else '<UNK>' \n",
    "                      for a in silver_authors]\n",
    "\n",
    "    gold_author_ints   = encoder.transform(gold_authors)\n",
    "    silver_author_ints = encoder.transform(silver_authors)\n",
    "\n",
    "    # get F1 for individual classes (and suppress warnings):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        f1 = f1_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        precision = precision_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        recall = recall_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        accuracy = accuracy_score(gold_author_ints,\n",
    "                  silver_author_ints)\n",
    "\n",
    "    return f1,precision,recall,accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ground_truth_file,predictions_file):\n",
    "    # Calculates evaluation measures for a single attribution problem\n",
    "    gt = {}\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        for attrib in json.load(f)['ground_truth']:\n",
    "            gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "    pred = {}\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            if attrib['unknown-text'] not in pred:\n",
    "                pred[attrib['unknown-text']] = attrib['predicted-author']\n",
    "    f1,precision,recall,accuracy =  eval_measures(gt,pred)\n",
    "    return f1, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "class DenseTransformer(BaseEstimator):\n",
    "    \"\"\"Convert a sparse array into a dense array.\"\"\"\n",
    "\n",
    "    def __init__(self, return_copy=True):\n",
    "        self.return_copy = return_copy\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\" Return a dense version of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_dense : dense version of the input X array.\n",
    "        \"\"\"\n",
    "        if issparse(X):\n",
    "            return X.toarray()\n",
    "        elif self.return_copy:\n",
    "            return X.copy()\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Mock method. Does nothing.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\" Return a dense version of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_dense : dense version of the input X array.\n",
    "        \"\"\"\n",
    "        return self.transform(X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### examinando o parametro min_df isoladamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runML(problem):\n",
    "    print (\"\\nProblem: %s,  language: %s, \" %(problem['problem'],problem['language']))\n",
    "    \n",
    "    train_docs, train_labels, _   = zip(*problem['candidates'])\n",
    "    problem['training_docs_size'] = len(train_docs);\n",
    "    test_docs, _, test_filename   = zip(*problem['unknown'])\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect',   TfidfVectorizer(analyzer='char',\n",
    "                                   min_df=0.05,\n",
    "                                   max_df=1.0,\n",
    "                                   norm='l1',\n",
    "                                   ngram_range=(3,5),\n",
    "                                   sublinear_tf=True,\n",
    "                                   smooth_idf=True,\n",
    "                                   lowercase =False)),\n",
    "        ('dense',  DenseTransformer()),\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('transf', PCA(0.999)),\n",
    "        ('clf', LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg')),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # uncommenting more parameters will give better exploring power but will\n",
    "    # increase processing time in a combinatorial way\n",
    "    parameters = {\n",
    "        'vect__min_df':(2,0.01,0.05,0.1)\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline,\n",
    "                               parameters,\n",
    "                               cv=5,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=False,\n",
    "                               scoring='f1_macro')\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    t0 = time()\n",
    "    grid_search.fit(train_docs, train_labels)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    train_pred=grid_search.predict(train_docs);\n",
    "    test_pred=grid_search.predict(test_docs);\n",
    "    \n",
    "    \n",
    "    # Writing output file\n",
    "    out_data=[]\n",
    "    for i,v in enumerate(test_pred):\n",
    "        out_data.append({'unknown-text': test_filename[i],'predicted-author': v})\n",
    "    answerFile = pathjoin(outputDir,'answers-'+problem['problem']+'.json');\n",
    "    with open(answerFile, 'w') as f:\n",
    "        json.dump(out_data, f, indent=4)\n",
    "    \n",
    "    \n",
    "    #evaluation train\n",
    "    f1,precision,recall,accuracy=evaluate(\n",
    "                pathjoin(inputDir, problem['problem'], 'ground-truth.json'),\n",
    "                answerFile)\n",
    "    \n",
    "    return {\n",
    "                'problem-name'  :       problem['problem'],\n",
    "                \"language\"      :       problem['language'],\n",
    "                'AuthorCount'   :       len(set(train_labels)),\n",
    "                \"train_doc_size\":       len(train_docs),\n",
    "                \"train_caract_per_doc\": sum([len(l) for l in train_docs])/len(train_docs),\n",
    "                \"test_doc_size\" :       len(test_docs),\n",
    "                \"test_caract_per_doc\":  sum([len(l) for l in test_docs])/len(test_docs),\n",
    "                \n",
    "                'macro-f1'       : round(f1,3),\n",
    "                'macro-precision': round(precision,3),\n",
    "                'macro-recall'   : round(recall,3),\n",
    "                'micro-accuracy' : round(accuracy,3),\n",
    "                \n",
    "        }, grid_search.cv_results_, best_parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem: problem00001,  language: en, \n",
      "Performing grid search...\n",
      "done in 38.968s\n",
      "Best score: 0.769\n",
      "Best parameters set:\n",
      "\tvect__min_df: 0.01\n",
      "\n",
      "Problem: problem00002,  language: en, \n",
      "Performing grid search...\n",
      "done in 29.814s\n",
      "Best score: 0.874\n",
      "Best parameters set:\n",
      "\tvect__min_df: 0.1\n",
      "\n",
      "Problem: problem00003,  language: fr, \n",
      "Performing grid search...\n",
      "done in 89.584s\n",
      "Best score: 0.775\n",
      "Best parameters set:\n",
      "\tvect__min_df: 0.01\n",
      "\n",
      "Problem: problem00004,  language: fr, \n",
      "Performing grid search...\n",
      "done in 31.481s\n",
      "Best score: 0.903\n",
      "Best parameters set:\n",
      "\tvect__min_df: 0.01\n",
      "\n",
      "Problem: problem00005,  language: it, \n",
      "Performing grid search...\n",
      "done in 91.047s\n",
      "Best score: 0.743\n",
      "Best parameters set:\n",
      "\tvect__min_df: 2\n",
      "\n",
      "Problem: problem00006,  language: it, \n",
      "Performing grid search...\n",
      "done in 33.172s\n",
      "Best score: 0.970\n",
      "Best parameters set:\n",
      "\tvect__min_df: 2\n",
      "\n",
      "Problem: problem00007,  language: pl, \n",
      "Performing grid search...\n",
      "done in 135.618s\n",
      "Best score: 0.811\n",
      "Best parameters set:\n",
      "\tvect__min_df: 0.01\n",
      "\n",
      "Problem: problem00008,  language: pl, \n",
      "Performing grid search...\n",
      "done in 49.869s\n",
      "Best score: 0.851\n",
      "Best parameters set:\n",
      "\tvect__min_df: 0.1\n",
      "\n",
      "Problem: problem00009,  language: sp, \n",
      "Performing grid search...\n",
      "done in 104.835s\n",
      "Best score: 0.917\n",
      "Best parameters set:\n",
      "\tvect__min_df: 0.01\n",
      "\n",
      "Problem: problem00010,  language: sp, \n",
      "Performing grid search...\n",
      "done in 37.666s\n",
      "Best score: 0.893\n",
      "Best parameters set:\n",
      "\tvect__min_df: 2\n"
     ]
    }
   ],
   "source": [
    "result = [];\n",
    "cv_result = [];\n",
    "best_parameters = [];\n",
    "for problem in problems:\n",
    "    r, c, b = runML(problem);\n",
    "    result.append(r);\n",
    "    cv_result.append(c);\n",
    "    b['problem'] = problem['problem'];\n",
    "    best_parameters.append(b);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>vect__min_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>problem00003</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>problem00004</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>problem00005</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>problem00006</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>problem00007</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>problem00008</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>problem00009</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>problem00010</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        problem  vect__min_df\n",
       "0  problem00001          0.01\n",
       "1  problem00002          0.10\n",
       "2  problem00003          0.01\n",
       "3  problem00004          0.01\n",
       "4  problem00005          2.00\n",
       "5  problem00006          2.00\n",
       "6  problem00007          0.01\n",
       "7  problem00008          0.10\n",
       "8  problem00009          0.01\n",
       "9  problem00010          2.00"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(best_parameters)[['problem','vect__min_df']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analisando os demais parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runML(problem):\n",
    "    print (\"\\nProblem: %s,  language: %s, \" %(problem['problem'],problem['language']))\n",
    "    \n",
    "    train_docs, train_labels, _   = zip(*problem['candidates'])\n",
    "    problem['training_docs_size'] = len(train_docs);\n",
    "    test_docs, _, test_filename   = zip(*problem['unknown'])\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect',   TfidfVectorizer(analyzer='char',\n",
    "                                   min_df=0.01,\n",
    "                                   max_df=1.0,\n",
    "                                   norm='l1',\n",
    "                                   lowercase =False,\n",
    "                                   sublinear_tf=True)),\n",
    "        ('dense',  DenseTransformer()),\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('transf', PCA()),\n",
    "        ('clf', LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg')),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # uncommenting more parameters will give better exploring power but will\n",
    "    # increase processing time in a combinatorial way\n",
    "    parameters = {\n",
    "        'vect__ngram_range':((2,3),(2,4),(2,5),(3,5)),\n",
    "        'vect__sublinear_tf':(True, False),\n",
    "        'vect__norm':('l1','l2'),\n",
    "        'transf__n_components': (0.1,0.25,0.5,0.75,0.9,0.99),\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline,\n",
    "                               parameters,\n",
    "                               cv=3,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=False,\n",
    "                               scoring='f1_macro')\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    t0 = time()\n",
    "    grid_search.fit(train_docs, train_labels)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    train_pred=grid_search.predict(train_docs);\n",
    "    test_pred=grid_search.predict(test_docs);\n",
    "    \n",
    "    \n",
    "    # Writing output file\n",
    "    out_data=[]\n",
    "    for i,v in enumerate(test_pred):\n",
    "        out_data.append({'unknown-text': test_filename[i],'predicted-author': v})\n",
    "    answerFile = pathjoin(outputDir,'answers-'+problem['problem']+'.json');\n",
    "    with open(answerFile, 'w') as f:\n",
    "        json.dump(out_data, f, indent=4)\n",
    "    \n",
    "    \n",
    "    #evaluation train\n",
    "    f1,precision,recall,accuracy=evaluate(\n",
    "                pathjoin(inputDir, problem['problem'], 'ground-truth.json'),\n",
    "                answerFile)\n",
    "    \n",
    "    return {\n",
    "                'problem-name'  :       problem['problem'],\n",
    "                \"language\"      :       problem['language'],\n",
    "                'AuthorCount'   :       len(set(train_labels)),\n",
    "                \"train_doc_size\":       len(train_docs),\n",
    "                \"train_caract_per_doc\": sum([len(l) for l in train_docs])/len(train_docs),\n",
    "                \"test_doc_size\" :       len(test_docs),\n",
    "                \"test_caract_per_doc\":  sum([len(l) for l in test_docs])/len(test_docs),\n",
    "                \n",
    "                'macro-f1'       : round(f1,3),\n",
    "                'macro-precision': round(precision,3),\n",
    "                'macro-recall'   : round(recall,3),\n",
    "                'micro-accuracy' : round(accuracy,3),\n",
    "                \n",
    "        }, grid_search.cv_results_,best_parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem: problem00001,  language: en, \n",
      "Performing grid search...\n",
      "done in 658.937s\n",
      "Best score: 0.833\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (2, 5)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__sublinear_tf: True\n",
      "\n",
      "Problem: problem00002,  language: en, \n",
      "Performing grid search...\n",
      "done in 123.030s\n",
      "Best score: 0.971\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.75\n",
      "\tvect__ngram_range: (2, 4)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__sublinear_tf: True\n",
      "\n",
      "Problem: problem00003,  language: fr, \n",
      "Performing grid search...\n",
      "done in 675.776s\n",
      "Best score: 0.800\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (2, 3)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__sublinear_tf: False\n",
      "\n",
      "Problem: problem00004,  language: fr, \n",
      "Performing grid search...\n",
      "done in 143.218s\n",
      "Best score: 0.854\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.75\n",
      "\tvect__ngram_range: (2, 4)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__sublinear_tf: True\n",
      "\n",
      "Problem: problem00005,  language: it, \n",
      "Performing grid search...\n",
      "done in 837.817s\n",
      "Best score: 0.701\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.75\n",
      "\tvect__ngram_range: (2, 3)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__sublinear_tf: True\n",
      "\n",
      "Problem: problem00006,  language: it, \n",
      "Performing grid search...\n",
      "done in 237.214s\n",
      "Best score: 0.971\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.9\n",
      "\tvect__ngram_range: (2, 4)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__sublinear_tf: True\n",
      "\n",
      "Problem: problem00007,  language: pl, \n",
      "Performing grid search...\n",
      "done in 1587.061s\n",
      "Best score: 0.816\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (2, 4)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__sublinear_tf: True\n",
      "\n",
      "Problem: problem00008,  language: pl, \n",
      "Performing grid search...\n",
      "done in 188.265s\n",
      "Best score: 0.845\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.9\n",
      "\tvect__ngram_range: (2, 4)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__sublinear_tf: True\n",
      "\n",
      "Problem: problem00009,  language: sp, \n",
      "Performing grid search...\n",
      "done in 866.830s\n",
      "Best score: 0.894\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (2, 5)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__sublinear_tf: True\n",
      "\n",
      "Problem: problem00010,  language: sp, \n",
      "Performing grid search...\n",
      "done in 171.587s\n",
      "Best score: 0.939\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (3, 5)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__sublinear_tf: False\n"
     ]
    }
   ],
   "source": [
    "result = [];\n",
    "cv_result = [];\n",
    "best_parameters = [];\n",
    "for problem in problems:\n",
    "    r, c, b = runML(problem);\n",
    "    result.append(r);\n",
    "    cv_result.append(c);\n",
    "    b['problem'] = problem['problem'];\n",
    "    best_parameters.append(b);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(result)[['problem-name',\n",
    "                     \"language\",\n",
    "                     'AuthorCount',\n",
    "                     \"train_doc_size\",\"train_caract_per_doc\",\n",
    "                     \"test_doc_size\", \"test_caract_per_doc\",\n",
    "                     'macro-f1','macro-precision','macro-recall' ,'micro-accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem-name</th>\n",
       "      <th>language</th>\n",
       "      <th>AuthorCount</th>\n",
       "      <th>train_doc_size</th>\n",
       "      <th>train_caract_per_doc</th>\n",
       "      <th>test_doc_size</th>\n",
       "      <th>test_caract_per_doc</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>macro-precision</th>\n",
       "      <th>macro-recall</th>\n",
       "      <th>micro-accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>problem00001</td>\n",
       "      <td>en</td>\n",
       "      <td>20</td>\n",
       "      <td>140</td>\n",
       "      <td>4327</td>\n",
       "      <td>105</td>\n",
       "      <td>4370</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>problem00002</td>\n",
       "      <td>en</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>4342</td>\n",
       "      <td>21</td>\n",
       "      <td>4296</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>problem00003</td>\n",
       "      <td>fr</td>\n",
       "      <td>20</td>\n",
       "      <td>140</td>\n",
       "      <td>4492</td>\n",
       "      <td>49</td>\n",
       "      <td>4508</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>problem00004</td>\n",
       "      <td>fr</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>4522</td>\n",
       "      <td>21</td>\n",
       "      <td>4532</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>problem00005</td>\n",
       "      <td>it</td>\n",
       "      <td>20</td>\n",
       "      <td>140</td>\n",
       "      <td>4720</td>\n",
       "      <td>80</td>\n",
       "      <td>4787</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>problem00006</td>\n",
       "      <td>it</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>4847</td>\n",
       "      <td>46</td>\n",
       "      <td>4765</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>problem00007</td>\n",
       "      <td>pl</td>\n",
       "      <td>20</td>\n",
       "      <td>140</td>\n",
       "      <td>5145</td>\n",
       "      <td>103</td>\n",
       "      <td>5200</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>problem00008</td>\n",
       "      <td>pl</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>5049</td>\n",
       "      <td>15</td>\n",
       "      <td>5214</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>problem00009</td>\n",
       "      <td>sp</td>\n",
       "      <td>20</td>\n",
       "      <td>140</td>\n",
       "      <td>4794</td>\n",
       "      <td>117</td>\n",
       "      <td>4788</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>problem00010</td>\n",
       "      <td>sp</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>4955</td>\n",
       "      <td>64</td>\n",
       "      <td>4827</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   problem-name language  AuthorCount  train_doc_size  train_caract_per_doc  \\\n",
       "0  problem00001       en           20             140                  4327   \n",
       "1  problem00002       en            5              35                  4342   \n",
       "2  problem00003       fr           20             140                  4492   \n",
       "3  problem00004       fr            5              35                  4522   \n",
       "4  problem00005       it           20             140                  4720   \n",
       "5  problem00006       it            5              35                  4847   \n",
       "6  problem00007       pl           20             140                  5145   \n",
       "7  problem00008       pl            5              35                  5049   \n",
       "8  problem00009       sp           20             140                  4794   \n",
       "9  problem00010       sp            5              35                  4955   \n",
       "\n",
       "   test_doc_size  test_caract_per_doc  macro-f1  macro-precision  \\\n",
       "0            105                 4370     0.643            0.649   \n",
       "1             21                 4296     0.477            0.517   \n",
       "2             49                 4508     0.641            0.667   \n",
       "3             21                 4532     0.747            0.767   \n",
       "4             80                 4787     0.481            0.483   \n",
       "5             46                 4765     0.596            0.596   \n",
       "6            103                 5200     0.465            0.490   \n",
       "7             15                 5214     0.822            0.800   \n",
       "8            117                 4788     0.787            0.788   \n",
       "9             64                 4827     0.832            0.857   \n",
       "\n",
       "   macro-recall  micro-accuracy  \n",
       "0         0.759           0.676  \n",
       "1         0.517           0.381  \n",
       "2         0.709           0.653  \n",
       "3         0.800           0.667  \n",
       "4         0.601           0.600  \n",
       "5         0.697           0.826  \n",
       "6         0.529           0.534  \n",
       "7         0.878           0.867  \n",
       "8         0.858           0.803  \n",
       "9         0.823           0.875  "
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rr}\n",
      "\\toprule\n",
      " index &  macro-f1 \\\\\n",
      "\\midrule\n",
      " 0 & 0.643 \\\\\n",
      " 1 & 0.477 \\\\\n",
      " 2 & 0.641 \\\\\n",
      " 3 & 0.747 \\\\\n",
      " 4 & 0.481 \\\\\n",
      " 5 & 0.596 \\\\\n",
      " 6 & 0.465 \\\\\n",
      " 7 & 0.822 \\\\\n",
      " 8 & 0.787 \\\\\n",
      " 9 & 0.832 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"macro-f1\"]].reset_index().to_latex(index=False).replace(\"     \",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages={\n",
    "    'en':'inglesa',\n",
    "    'sp':'espanhola',\n",
    "    'it':'italiana',\n",
    "    'pl':'polonesa',\n",
    "    'fr':'francesa'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result2 = [];\n",
    "dfCV = pd.DataFrame();\n",
    "for i, c in enumerate(cv_result):\n",
    "    temp = pd.DataFrame(c);\n",
    "    temp['problem'] = i+1;\n",
    "    temp['language'] = languages[problems[i]['language']]\n",
    "    dfCV = dfCV.append(temp);\n",
    "\n",
    "for p in ['param_transf__n_components',\n",
    "    'mean_test_score','std_test_score','mean_train_score',   \n",
    "    'split0_test_score','split0_train_score',\n",
    "    'split1_test_score','split1_train_score',\n",
    "    'split2_test_score','split2_train_score']:\n",
    "    dfCV[p]=dfCV[p].astype(np.float32);\n",
    "\n",
    "    \n",
    "dfCV =dfCV[[\n",
    "    'problem',\n",
    "    'language',\n",
    "    'rank_test_score',\n",
    "    'param_transf__n_components',\n",
    "    'param_vect__ngram_range',\n",
    "    'param_vect__sublinear_tf',\n",
    "    'param_vect__norm',\n",
    "    'mean_test_score',   \n",
    "    'std_test_score',\n",
    "    'mean_train_score',   \n",
    "\n",
    "    'split0_test_score','split0_train_score',\n",
    "    'split1_test_score','split1_train_score',\n",
    "    'split2_test_score','split2_train_score',\n",
    "\n",
    "    'mean_score_time',\n",
    "    'mean_fit_time',\n",
    "    'std_fit_time',\n",
    "    'std_score_time',\n",
    "    'std_train_score',\n",
    "]];\n",
    "\n",
    "dfCV.rename(columns={\n",
    "    'param_transf__n_components':'PCA_componentes',\n",
    "    'param_vect__ngram_range':'ngram_range',\n",
    "    'param_vect__sublinear_tf':'sublinear_tf',\n",
    "    'param_vect__smooth_idf':'smooth_idf',\n",
    "    'param_vect__norm':'norm'\n",
    "},inplace=True);\n",
    "\n",
    "#print('\\',\\n\\''.join(dfCV.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCV.to_csv('PANAA2018_CHAR.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCV = pd.read_csv('PANAA2018_CHAR.csv', na_values='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>language</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>sublinear_tf</th>\n",
       "      <th>norm</th>\n",
       "      <th>PCA_componentes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>inglesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833299</td>\n",
       "      <td>0.020391</td>\n",
       "      <td>(2, 5)</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2</td>\n",
       "      <td>inglesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2</td>\n",
       "      <td>inglesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2</td>\n",
       "      <td>inglesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>False</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2</td>\n",
       "      <td>inglesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(2, 5)</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2</td>\n",
       "      <td>inglesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(2, 5)</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2</td>\n",
       "      <td>inglesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2</td>\n",
       "      <td>inglesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2</td>\n",
       "      <td>inglesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2</td>\n",
       "      <td>inglesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>3</td>\n",
       "      <td>francesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800204</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>False</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>4</td>\n",
       "      <td>francesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.066737</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>4</td>\n",
       "      <td>francesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.066737</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>4</td>\n",
       "      <td>francesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.066737</td>\n",
       "      <td>(2, 5)</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>4</td>\n",
       "      <td>francesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.066737</td>\n",
       "      <td>(2, 5)</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>4</td>\n",
       "      <td>francesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.036396</td>\n",
       "      <td>(2, 5)</td>\n",
       "      <td>False</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>4</td>\n",
       "      <td>francesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.066737</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>4</td>\n",
       "      <td>francesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.036396</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>5</td>\n",
       "      <td>italiana</td>\n",
       "      <td>1</td>\n",
       "      <td>0.701003</td>\n",
       "      <td>0.097444</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>6</td>\n",
       "      <td>italiana</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>6</td>\n",
       "      <td>italiana</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>6</td>\n",
       "      <td>italiana</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>6</td>\n",
       "      <td>italiana</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>False</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>6</td>\n",
       "      <td>italiana</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>False</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>7</td>\n",
       "      <td>polonesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.816241</td>\n",
       "      <td>0.049464</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>8</td>\n",
       "      <td>polonesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845374</td>\n",
       "      <td>0.055378</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>False</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>8</td>\n",
       "      <td>polonesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845374</td>\n",
       "      <td>0.055378</td>\n",
       "      <td>(2, 5)</td>\n",
       "      <td>False</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>8</td>\n",
       "      <td>polonesa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845374</td>\n",
       "      <td>0.055378</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>9</td>\n",
       "      <td>espanhola</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894252</td>\n",
       "      <td>0.038470</td>\n",
       "      <td>(2, 5)</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>10</td>\n",
       "      <td>espanhola</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939048</td>\n",
       "      <td>0.052786</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>False</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     problem   language  rank_test_score  mean_test_score  std_test_score  \\\n",
       "88         1    inglesa                1         0.833299        0.020391   \n",
       "188        2    inglesa                1         0.970612        0.033934   \n",
       "190        2    inglesa                1         0.970612        0.033934   \n",
       "189        2    inglesa                1         0.970612        0.033934   \n",
       "184        2    inglesa                1         0.970612        0.033934   \n",
       "186        2    inglesa                1         0.970612        0.033934   \n",
       "180        2    inglesa                1         0.970612        0.033934   \n",
       "182        2    inglesa                1         0.970612        0.033934   \n",
       "164        2    inglesa                1         0.970612        0.033934   \n",
       "148        2    inglesa                1         0.970612        0.033934   \n",
       "273        3   francesa                1         0.800204        0.031250   \n",
       "380        4   francesa                1         0.854422        0.066737   \n",
       "382        4   francesa                1         0.854422        0.066737   \n",
       "376        4   francesa                1         0.854422        0.066737   \n",
       "378        4   francesa                1         0.854422        0.066737   \n",
       "345        4   francesa                1         0.854422        0.036396   \n",
       "358        4   francesa                1         0.854422        0.066737   \n",
       "342        4   francesa                1         0.854422        0.036396   \n",
       "434        5   italiana                1         0.701003        0.097444   \n",
       "566        6   italiana                1         0.970612        0.033934   \n",
       "548        6   italiana                1         0.970612        0.033934   \n",
       "550        6   italiana                1         0.970612        0.033934   \n",
       "565        6   italiana                1         0.970612        0.033934   \n",
       "567        6   italiana                1         0.970612        0.033934   \n",
       "660        7   polonesa                1         0.816241        0.049464   \n",
       "767        8   polonesa                1         0.845374        0.055378   \n",
       "763        8   polonesa                1         0.845374        0.055378   \n",
       "742        8   polonesa                1         0.845374        0.055378   \n",
       "856        9  espanhola                1         0.894252        0.038470   \n",
       "959       10  espanhola                1         0.939048        0.052786   \n",
       "\n",
       "    ngram_range  sublinear_tf norm  PCA_componentes  \n",
       "88       (2, 5)          True   l1             0.99  \n",
       "188      (3, 5)          True   l1             0.99  \n",
       "190      (3, 5)          True   l2             0.99  \n",
       "189      (3, 5)         False   l1             0.99  \n",
       "184      (2, 5)          True   l1             0.99  \n",
       "186      (2, 5)          True   l2             0.99  \n",
       "180      (2, 4)          True   l1             0.99  \n",
       "182      (2, 4)          True   l2             0.99  \n",
       "164      (2, 4)          True   l1             0.90  \n",
       "148      (2, 4)          True   l1             0.75  \n",
       "273      (2, 3)         False   l1             0.99  \n",
       "380      (3, 5)          True   l1             0.99  \n",
       "382      (3, 5)          True   l2             0.99  \n",
       "376      (2, 5)          True   l1             0.99  \n",
       "378      (2, 5)          True   l2             0.99  \n",
       "345      (2, 5)         False   l1             0.75  \n",
       "358      (2, 4)          True   l2             0.90  \n",
       "342      (2, 4)          True   l2             0.75  \n",
       "434      (2, 3)          True   l2             0.75  \n",
       "566      (2, 4)          True   l2             0.99  \n",
       "548      (2, 4)          True   l1             0.90  \n",
       "550      (2, 4)          True   l2             0.90  \n",
       "565      (2, 4)         False   l1             0.99  \n",
       "567      (2, 4)         False   l2             0.99  \n",
       "660      (2, 4)          True   l1             0.99  \n",
       "767      (3, 5)         False   l2             0.99  \n",
       "763      (2, 5)         False   l2             0.99  \n",
       "742      (2, 4)          True   l2             0.90  \n",
       "856      (2, 5)          True   l1             0.99  \n",
       "959      (3, 5)         False   l2             0.99  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dfCV[dfCV.rank_test_score == 1])[\n",
    "    ['problem',\n",
    "     'language',\n",
    "    'rank_test_score',\n",
    "    'mean_test_score',\n",
    "    'std_test_score',\n",
    "    'ngram_range',\n",
    "    'sublinear_tf',\n",
    "     'norm',\n",
    "    'PCA_componentes']\n",
    "].sort_values(by=[\n",
    "    'problem',\n",
    "    'mean_test_score',\n",
    "    'ngram_range',\n",
    "    'sublinear_tf',\n",
    "    'PCA_componentes'\n",
    "], ascending=[True, False,False,False,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>norm</th>\n",
       "      <th colspan=\"8\" halign=\"left\">l1</th>\n",
       "      <th colspan=\"8\" halign=\"left\">l2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sublinear_tf</th>\n",
       "      <th colspan=\"4\" halign=\"left\">False</th>\n",
       "      <th colspan=\"4\" halign=\"left\">True</th>\n",
       "      <th colspan=\"4\" halign=\"left\">False</th>\n",
       "      <th colspan=\"4\" halign=\"left\">True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>(2, 3)</th>\n",
       "      <th>(2, 4)</th>\n",
       "      <th>(2, 5)</th>\n",
       "      <th>(3, 5)</th>\n",
       "      <th>(2, 3)</th>\n",
       "      <th>(2, 4)</th>\n",
       "      <th>(2, 5)</th>\n",
       "      <th>(3, 5)</th>\n",
       "      <th>(2, 3)</th>\n",
       "      <th>(2, 4)</th>\n",
       "      <th>(2, 5)</th>\n",
       "      <th>(3, 5)</th>\n",
       "      <th>(2, 3)</th>\n",
       "      <th>(2, 4)</th>\n",
       "      <th>(2, 5)</th>\n",
       "      <th>(3, 5)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <th>language</th>\n",
       "      <th>PCA_componentes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">inglesa</th>\n",
       "      <th>0.10</th>\n",
       "      <td>0.289050</td>\n",
       "      <td>0.213501</td>\n",
       "      <td>0.135221</td>\n",
       "      <td>0.144955</td>\n",
       "      <td>0.247157</td>\n",
       "      <td>0.202389</td>\n",
       "      <td>0.111574</td>\n",
       "      <td>0.105737</td>\n",
       "      <td>0.261712</td>\n",
       "      <td>0.147346</td>\n",
       "      <td>0.157838</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0.225762</td>\n",
       "      <td>0.201633</td>\n",
       "      <td>0.175090</td>\n",
       "      <td>0.155582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.475246</td>\n",
       "      <td>0.473339</td>\n",
       "      <td>0.432229</td>\n",
       "      <td>0.486210</td>\n",
       "      <td>0.566849</td>\n",
       "      <td>0.525181</td>\n",
       "      <td>0.573929</td>\n",
       "      <td>0.356450</td>\n",
       "      <td>0.344444</td>\n",
       "      <td>0.244823</td>\n",
       "      <td>0.245465</td>\n",
       "      <td>0.469694</td>\n",
       "      <td>0.452204</td>\n",
       "      <td>0.433352</td>\n",
       "      <td>0.397762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.587794</td>\n",
       "      <td>0.633503</td>\n",
       "      <td>0.690323</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>0.709813</td>\n",
       "      <td>0.784048</td>\n",
       "      <td>0.788146</td>\n",
       "      <td>0.789694</td>\n",
       "      <td>0.530153</td>\n",
       "      <td>0.415823</td>\n",
       "      <td>0.346530</td>\n",
       "      <td>0.319832</td>\n",
       "      <td>0.697823</td>\n",
       "      <td>0.729201</td>\n",
       "      <td>0.706701</td>\n",
       "      <td>0.723929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.640255</td>\n",
       "      <td>0.734388</td>\n",
       "      <td>0.767228</td>\n",
       "      <td>0.759660</td>\n",
       "      <td>0.742721</td>\n",
       "      <td>0.792092</td>\n",
       "      <td>0.790527</td>\n",
       "      <td>0.792568</td>\n",
       "      <td>0.591655</td>\n",
       "      <td>0.600161</td>\n",
       "      <td>0.603951</td>\n",
       "      <td>0.493628</td>\n",
       "      <td>0.735527</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.781173</td>\n",
       "      <td>0.747840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.668231</td>\n",
       "      <td>0.783061</td>\n",
       "      <td>0.811412</td>\n",
       "      <td>0.795527</td>\n",
       "      <td>0.772653</td>\n",
       "      <td>0.799745</td>\n",
       "      <td>0.813895</td>\n",
       "      <td>0.806548</td>\n",
       "      <td>0.650258</td>\n",
       "      <td>0.718389</td>\n",
       "      <td>0.614846</td>\n",
       "      <td>0.588564</td>\n",
       "      <td>0.794252</td>\n",
       "      <td>0.817925</td>\n",
       "      <td>0.792619</td>\n",
       "      <td>0.794422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.680153</td>\n",
       "      <td>0.786650</td>\n",
       "      <td>0.803554</td>\n",
       "      <td>0.803350</td>\n",
       "      <td>0.791224</td>\n",
       "      <td>0.826905</td>\n",
       "      <td>0.833299</td>\n",
       "      <td>0.826718</td>\n",
       "      <td>0.669558</td>\n",
       "      <td>0.759524</td>\n",
       "      <td>0.714898</td>\n",
       "      <td>0.734116</td>\n",
       "      <td>0.816037</td>\n",
       "      <td>0.817925</td>\n",
       "      <td>0.818844</td>\n",
       "      <td>0.826463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">2</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">inglesa</th>\n",
       "      <th>0.10</th>\n",
       "      <td>0.484966</td>\n",
       "      <td>0.406753</td>\n",
       "      <td>0.359320</td>\n",
       "      <td>0.316871</td>\n",
       "      <td>0.380245</td>\n",
       "      <td>0.357388</td>\n",
       "      <td>0.178926</td>\n",
       "      <td>0.177547</td>\n",
       "      <td>0.357619</td>\n",
       "      <td>0.321465</td>\n",
       "      <td>0.171190</td>\n",
       "      <td>0.207446</td>\n",
       "      <td>0.380245</td>\n",
       "      <td>0.321991</td>\n",
       "      <td>0.275873</td>\n",
       "      <td>0.253968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.760476</td>\n",
       "      <td>0.564762</td>\n",
       "      <td>0.518095</td>\n",
       "      <td>0.487619</td>\n",
       "      <td>0.707143</td>\n",
       "      <td>0.648095</td>\n",
       "      <td>0.681905</td>\n",
       "      <td>0.611787</td>\n",
       "      <td>0.635374</td>\n",
       "      <td>0.438730</td>\n",
       "      <td>0.428540</td>\n",
       "      <td>0.339048</td>\n",
       "      <td>0.722313</td>\n",
       "      <td>0.649932</td>\n",
       "      <td>0.491198</td>\n",
       "      <td>0.445121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.885238</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.802993</td>\n",
       "      <td>0.802993</td>\n",
       "      <td>0.885238</td>\n",
       "      <td>0.831837</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.854762</td>\n",
       "      <td>0.738231</td>\n",
       "      <td>0.643537</td>\n",
       "      <td>0.553878</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.879184</td>\n",
       "      <td>0.810884</td>\n",
       "      <td>0.839456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.852857</td>\n",
       "      <td>0.882993</td>\n",
       "      <td>0.882993</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.795714</td>\n",
       "      <td>0.784422</td>\n",
       "      <td>0.747279</td>\n",
       "      <td>0.492064</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.845034</td>\n",
       "      <td>0.882993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.854762</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.882993</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.880272</td>\n",
       "      <td>0.802993</td>\n",
       "      <td>0.721497</td>\n",
       "      <td>0.593469</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.882993</td>\n",
       "      <td>0.882993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.879184</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.970612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">francesa</th>\n",
       "      <th>0.10</th>\n",
       "      <td>0.270412</td>\n",
       "      <td>0.167608</td>\n",
       "      <td>0.158879</td>\n",
       "      <td>0.162775</td>\n",
       "      <td>0.204083</td>\n",
       "      <td>0.170256</td>\n",
       "      <td>0.114198</td>\n",
       "      <td>0.122971</td>\n",
       "      <td>0.238987</td>\n",
       "      <td>0.204862</td>\n",
       "      <td>0.142976</td>\n",
       "      <td>0.143707</td>\n",
       "      <td>0.209215</td>\n",
       "      <td>0.205600</td>\n",
       "      <td>0.170882</td>\n",
       "      <td>0.168999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.471545</td>\n",
       "      <td>0.475370</td>\n",
       "      <td>0.456113</td>\n",
       "      <td>0.464135</td>\n",
       "      <td>0.545282</td>\n",
       "      <td>0.606876</td>\n",
       "      <td>0.584609</td>\n",
       "      <td>0.559626</td>\n",
       "      <td>0.431922</td>\n",
       "      <td>0.405226</td>\n",
       "      <td>0.300296</td>\n",
       "      <td>0.318425</td>\n",
       "      <td>0.535779</td>\n",
       "      <td>0.531665</td>\n",
       "      <td>0.480772</td>\n",
       "      <td>0.475074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.708559</td>\n",
       "      <td>0.756378</td>\n",
       "      <td>0.691395</td>\n",
       "      <td>0.694847</td>\n",
       "      <td>0.746003</td>\n",
       "      <td>0.771156</td>\n",
       "      <td>0.787160</td>\n",
       "      <td>0.767602</td>\n",
       "      <td>0.656701</td>\n",
       "      <td>0.553418</td>\n",
       "      <td>0.441724</td>\n",
       "      <td>0.399718</td>\n",
       "      <td>0.744269</td>\n",
       "      <td>0.741054</td>\n",
       "      <td>0.757160</td>\n",
       "      <td>0.723639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.772194</td>\n",
       "      <td>0.746259</td>\n",
       "      <td>0.738214</td>\n",
       "      <td>0.714065</td>\n",
       "      <td>0.740901</td>\n",
       "      <td>0.778878</td>\n",
       "      <td>0.767908</td>\n",
       "      <td>0.759388</td>\n",
       "      <td>0.722483</td>\n",
       "      <td>0.668066</td>\n",
       "      <td>0.411551</td>\n",
       "      <td>0.430339</td>\n",
       "      <td>0.747670</td>\n",
       "      <td>0.758878</td>\n",
       "      <td>0.754218</td>\n",
       "      <td>0.759932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.769881</td>\n",
       "      <td>0.768180</td>\n",
       "      <td>0.783741</td>\n",
       "      <td>0.768792</td>\n",
       "      <td>0.778759</td>\n",
       "      <td>0.779949</td>\n",
       "      <td>0.756105</td>\n",
       "      <td>0.762296</td>\n",
       "      <td>0.762194</td>\n",
       "      <td>0.695867</td>\n",
       "      <td>0.427517</td>\n",
       "      <td>0.495781</td>\n",
       "      <td>0.783929</td>\n",
       "      <td>0.774983</td>\n",
       "      <td>0.755034</td>\n",
       "      <td>0.752262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.800204</td>\n",
       "      <td>0.781599</td>\n",
       "      <td>0.772483</td>\n",
       "      <td>0.771531</td>\n",
       "      <td>0.777942</td>\n",
       "      <td>0.788078</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.775272</td>\n",
       "      <td>0.794184</td>\n",
       "      <td>0.760782</td>\n",
       "      <td>0.732101</td>\n",
       "      <td>0.723802</td>\n",
       "      <td>0.786310</td>\n",
       "      <td>0.768537</td>\n",
       "      <td>0.763452</td>\n",
       "      <td>0.776020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">francesa</th>\n",
       "      <th>0.10</th>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.200779</td>\n",
       "      <td>0.232894</td>\n",
       "      <td>0.206111</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.162313</td>\n",
       "      <td>0.182004</td>\n",
       "      <td>0.168911</td>\n",
       "      <td>0.210159</td>\n",
       "      <td>0.243307</td>\n",
       "      <td>0.232072</td>\n",
       "      <td>0.214062</td>\n",
       "      <td>0.221905</td>\n",
       "      <td>0.182313</td>\n",
       "      <td>0.201884</td>\n",
       "      <td>0.201939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.357857</td>\n",
       "      <td>0.384005</td>\n",
       "      <td>0.455007</td>\n",
       "      <td>0.455007</td>\n",
       "      <td>0.321148</td>\n",
       "      <td>0.394353</td>\n",
       "      <td>0.557415</td>\n",
       "      <td>0.557415</td>\n",
       "      <td>0.348398</td>\n",
       "      <td>0.485065</td>\n",
       "      <td>0.501039</td>\n",
       "      <td>0.432381</td>\n",
       "      <td>0.377056</td>\n",
       "      <td>0.318961</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.428341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.648866</td>\n",
       "      <td>0.644762</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.743401</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.792993</td>\n",
       "      <td>0.792993</td>\n",
       "      <td>0.574286</td>\n",
       "      <td>0.564286</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.578182</td>\n",
       "      <td>0.711020</td>\n",
       "      <td>0.703333</td>\n",
       "      <td>0.582540</td>\n",
       "      <td>0.604762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.739932</td>\n",
       "      <td>0.792993</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.825034</td>\n",
       "      <td>0.735170</td>\n",
       "      <td>0.825850</td>\n",
       "      <td>0.825034</td>\n",
       "      <td>0.825034</td>\n",
       "      <td>0.768503</td>\n",
       "      <td>0.804898</td>\n",
       "      <td>0.704558</td>\n",
       "      <td>0.654558</td>\n",
       "      <td>0.799524</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.818707</td>\n",
       "      <td>0.780748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.775238</td>\n",
       "      <td>0.799524</td>\n",
       "      <td>0.796463</td>\n",
       "      <td>0.796463</td>\n",
       "      <td>0.743810</td>\n",
       "      <td>0.825850</td>\n",
       "      <td>0.825034</td>\n",
       "      <td>0.825034</td>\n",
       "      <td>0.743810</td>\n",
       "      <td>0.778571</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.807415</td>\n",
       "      <td>0.772381</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.825034</td>\n",
       "      <td>0.790136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.775238</td>\n",
       "      <td>0.799524</td>\n",
       "      <td>0.825034</td>\n",
       "      <td>0.825034</td>\n",
       "      <td>0.743810</td>\n",
       "      <td>0.799524</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.743810</td>\n",
       "      <td>0.799524</td>\n",
       "      <td>0.853605</td>\n",
       "      <td>0.853605</td>\n",
       "      <td>0.799048</td>\n",
       "      <td>0.828095</td>\n",
       "      <td>0.854422</td>\n",
       "      <td>0.854422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">italiana</th>\n",
       "      <th>0.10</th>\n",
       "      <td>0.251173</td>\n",
       "      <td>0.216178</td>\n",
       "      <td>0.145058</td>\n",
       "      <td>0.139421</td>\n",
       "      <td>0.244575</td>\n",
       "      <td>0.197154</td>\n",
       "      <td>0.101712</td>\n",
       "      <td>0.102508</td>\n",
       "      <td>0.286071</td>\n",
       "      <td>0.177957</td>\n",
       "      <td>0.131691</td>\n",
       "      <td>0.124456</td>\n",
       "      <td>0.241795</td>\n",
       "      <td>0.193724</td>\n",
       "      <td>0.115280</td>\n",
       "      <td>0.139841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.418810</td>\n",
       "      <td>0.447969</td>\n",
       "      <td>0.409962</td>\n",
       "      <td>0.396136</td>\n",
       "      <td>0.444544</td>\n",
       "      <td>0.376460</td>\n",
       "      <td>0.383626</td>\n",
       "      <td>0.390683</td>\n",
       "      <td>0.439610</td>\n",
       "      <td>0.401461</td>\n",
       "      <td>0.332051</td>\n",
       "      <td>0.391267</td>\n",
       "      <td>0.451446</td>\n",
       "      <td>0.490510</td>\n",
       "      <td>0.482947</td>\n",
       "      <td>0.469167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.628690</td>\n",
       "      <td>0.574171</td>\n",
       "      <td>0.529200</td>\n",
       "      <td>0.546047</td>\n",
       "      <td>0.625017</td>\n",
       "      <td>0.629626</td>\n",
       "      <td>0.639205</td>\n",
       "      <td>0.605091</td>\n",
       "      <td>0.630935</td>\n",
       "      <td>0.488163</td>\n",
       "      <td>0.402989</td>\n",
       "      <td>0.403326</td>\n",
       "      <td>0.630374</td>\n",
       "      <td>0.644426</td>\n",
       "      <td>0.606897</td>\n",
       "      <td>0.588666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.659881</td>\n",
       "      <td>0.625617</td>\n",
       "      <td>0.623764</td>\n",
       "      <td>0.624443</td>\n",
       "      <td>0.688793</td>\n",
       "      <td>0.657840</td>\n",
       "      <td>0.662126</td>\n",
       "      <td>0.644346</td>\n",
       "      <td>0.593423</td>\n",
       "      <td>0.534399</td>\n",
       "      <td>0.386599</td>\n",
       "      <td>0.420805</td>\n",
       "      <td>0.701003</td>\n",
       "      <td>0.665425</td>\n",
       "      <td>0.670192</td>\n",
       "      <td>0.643741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.660391</td>\n",
       "      <td>0.634120</td>\n",
       "      <td>0.632602</td>\n",
       "      <td>0.619031</td>\n",
       "      <td>0.690102</td>\n",
       "      <td>0.672058</td>\n",
       "      <td>0.672126</td>\n",
       "      <td>0.624558</td>\n",
       "      <td>0.622835</td>\n",
       "      <td>0.541967</td>\n",
       "      <td>0.404153</td>\n",
       "      <td>0.425676</td>\n",
       "      <td>0.675459</td>\n",
       "      <td>0.691514</td>\n",
       "      <td>0.686003</td>\n",
       "      <td>0.699167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.653639</td>\n",
       "      <td>0.628810</td>\n",
       "      <td>0.642823</td>\n",
       "      <td>0.651173</td>\n",
       "      <td>0.680578</td>\n",
       "      <td>0.664898</td>\n",
       "      <td>0.676735</td>\n",
       "      <td>0.665816</td>\n",
       "      <td>0.628469</td>\n",
       "      <td>0.550351</td>\n",
       "      <td>0.392630</td>\n",
       "      <td>0.409102</td>\n",
       "      <td>0.681854</td>\n",
       "      <td>0.688469</td>\n",
       "      <td>0.658775</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">6</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">italiana</th>\n",
       "      <th>0.10</th>\n",
       "      <td>0.344898</td>\n",
       "      <td>0.241088</td>\n",
       "      <td>0.232344</td>\n",
       "      <td>0.225541</td>\n",
       "      <td>0.332032</td>\n",
       "      <td>0.188571</td>\n",
       "      <td>0.117698</td>\n",
       "      <td>0.145173</td>\n",
       "      <td>0.529765</td>\n",
       "      <td>0.281667</td>\n",
       "      <td>0.126176</td>\n",
       "      <td>0.126176</td>\n",
       "      <td>0.297936</td>\n",
       "      <td>0.172511</td>\n",
       "      <td>0.133095</td>\n",
       "      <td>0.181349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.657687</td>\n",
       "      <td>0.464444</td>\n",
       "      <td>0.425714</td>\n",
       "      <td>0.412857</td>\n",
       "      <td>0.810563</td>\n",
       "      <td>0.656599</td>\n",
       "      <td>0.523129</td>\n",
       "      <td>0.523129</td>\n",
       "      <td>0.836463</td>\n",
       "      <td>0.464725</td>\n",
       "      <td>0.280224</td>\n",
       "      <td>0.494921</td>\n",
       "      <td>0.707706</td>\n",
       "      <td>0.684422</td>\n",
       "      <td>0.632925</td>\n",
       "      <td>0.676463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.910748</td>\n",
       "      <td>0.875850</td>\n",
       "      <td>0.880272</td>\n",
       "      <td>0.849796</td>\n",
       "      <td>0.910748</td>\n",
       "      <td>0.886258</td>\n",
       "      <td>0.855782</td>\n",
       "      <td>0.886258</td>\n",
       "      <td>0.880272</td>\n",
       "      <td>0.787891</td>\n",
       "      <td>0.698980</td>\n",
       "      <td>0.804898</td>\n",
       "      <td>0.941225</td>\n",
       "      <td>0.851701</td>\n",
       "      <td>0.881088</td>\n",
       "      <td>0.881088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.911565</td>\n",
       "      <td>0.881088</td>\n",
       "      <td>0.941225</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.910748</td>\n",
       "      <td>0.910748</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.780476</td>\n",
       "      <td>0.910748</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.909660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.880272</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.941225</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.880272</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.833469</td>\n",
       "      <td>0.881088</td>\n",
       "      <td>0.910748</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.909660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.910748</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.909660</td>\n",
       "      <td>0.880272</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.910748</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.940136</td>\n",
       "      <td>0.940136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">7</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">polonesa</th>\n",
       "      <th>0.10</th>\n",
       "      <td>0.292150</td>\n",
       "      <td>0.220749</td>\n",
       "      <td>0.163698</td>\n",
       "      <td>0.178481</td>\n",
       "      <td>0.354521</td>\n",
       "      <td>0.199089</td>\n",
       "      <td>0.183448</td>\n",
       "      <td>0.178918</td>\n",
       "      <td>0.305014</td>\n",
       "      <td>0.278441</td>\n",
       "      <td>0.137852</td>\n",
       "      <td>0.177517</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.196897</td>\n",
       "      <td>0.154359</td>\n",
       "      <td>0.183950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.651990</td>\n",
       "      <td>0.565428</td>\n",
       "      <td>0.624476</td>\n",
       "      <td>0.639136</td>\n",
       "      <td>0.695111</td>\n",
       "      <td>0.704972</td>\n",
       "      <td>0.722607</td>\n",
       "      <td>0.698151</td>\n",
       "      <td>0.581215</td>\n",
       "      <td>0.408031</td>\n",
       "      <td>0.224665</td>\n",
       "      <td>0.252983</td>\n",
       "      <td>0.676658</td>\n",
       "      <td>0.704995</td>\n",
       "      <td>0.599972</td>\n",
       "      <td>0.563194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.713302</td>\n",
       "      <td>0.748078</td>\n",
       "      <td>0.750459</td>\n",
       "      <td>0.739779</td>\n",
       "      <td>0.792721</td>\n",
       "      <td>0.794218</td>\n",
       "      <td>0.764881</td>\n",
       "      <td>0.752568</td>\n",
       "      <td>0.728316</td>\n",
       "      <td>0.611486</td>\n",
       "      <td>0.447133</td>\n",
       "      <td>0.424007</td>\n",
       "      <td>0.790816</td>\n",
       "      <td>0.788759</td>\n",
       "      <td>0.728027</td>\n",
       "      <td>0.706429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.750425</td>\n",
       "      <td>0.760068</td>\n",
       "      <td>0.806412</td>\n",
       "      <td>0.791293</td>\n",
       "      <td>0.804864</td>\n",
       "      <td>0.799745</td>\n",
       "      <td>0.815170</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.722913</td>\n",
       "      <td>0.673254</td>\n",
       "      <td>0.438658</td>\n",
       "      <td>0.446607</td>\n",
       "      <td>0.778469</td>\n",
       "      <td>0.779694</td>\n",
       "      <td>0.800136</td>\n",
       "      <td>0.793044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.782041</td>\n",
       "      <td>0.773350</td>\n",
       "      <td>0.777874</td>\n",
       "      <td>0.770255</td>\n",
       "      <td>0.786242</td>\n",
       "      <td>0.815289</td>\n",
       "      <td>0.792755</td>\n",
       "      <td>0.784031</td>\n",
       "      <td>0.750561</td>\n",
       "      <td>0.652215</td>\n",
       "      <td>0.528016</td>\n",
       "      <td>0.438444</td>\n",
       "      <td>0.764643</td>\n",
       "      <td>0.812483</td>\n",
       "      <td>0.791599</td>\n",
       "      <td>0.800323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.782296</td>\n",
       "      <td>0.792517</td>\n",
       "      <td>0.770255</td>\n",
       "      <td>0.770255</td>\n",
       "      <td>0.784065</td>\n",
       "      <td>0.816241</td>\n",
       "      <td>0.814065</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.748078</td>\n",
       "      <td>0.723106</td>\n",
       "      <td>0.712921</td>\n",
       "      <td>0.707415</td>\n",
       "      <td>0.784065</td>\n",
       "      <td>0.809507</td>\n",
       "      <td>0.814065</td>\n",
       "      <td>0.799099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">8</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">polonesa</th>\n",
       "      <th>0.10</th>\n",
       "      <td>0.153095</td>\n",
       "      <td>0.210037</td>\n",
       "      <td>0.143377</td>\n",
       "      <td>0.143377</td>\n",
       "      <td>0.154113</td>\n",
       "      <td>0.120513</td>\n",
       "      <td>0.143377</td>\n",
       "      <td>0.183203</td>\n",
       "      <td>0.130711</td>\n",
       "      <td>0.099048</td>\n",
       "      <td>0.159878</td>\n",
       "      <td>0.223946</td>\n",
       "      <td>0.118730</td>\n",
       "      <td>0.104524</td>\n",
       "      <td>0.143377</td>\n",
       "      <td>0.190043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.482944</td>\n",
       "      <td>0.688435</td>\n",
       "      <td>0.470037</td>\n",
       "      <td>0.373095</td>\n",
       "      <td>0.511429</td>\n",
       "      <td>0.542993</td>\n",
       "      <td>0.647075</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>0.321224</td>\n",
       "      <td>0.433968</td>\n",
       "      <td>0.202177</td>\n",
       "      <td>0.204538</td>\n",
       "      <td>0.462177</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.435952</td>\n",
       "      <td>0.344220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.564372</td>\n",
       "      <td>0.657229</td>\n",
       "      <td>0.567415</td>\n",
       "      <td>0.666191</td>\n",
       "      <td>0.710952</td>\n",
       "      <td>0.657229</td>\n",
       "      <td>0.506753</td>\n",
       "      <td>0.506753</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.451429</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.405102</td>\n",
       "      <td>0.657229</td>\n",
       "      <td>0.617415</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.692789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.711905</td>\n",
       "      <td>0.760476</td>\n",
       "      <td>0.632993</td>\n",
       "      <td>0.654422</td>\n",
       "      <td>0.760476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.733810</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.711905</td>\n",
       "      <td>0.690272</td>\n",
       "      <td>0.693469</td>\n",
       "      <td>0.604985</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.631088</td>\n",
       "      <td>0.631088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.711905</td>\n",
       "      <td>0.760476</td>\n",
       "      <td>0.678708</td>\n",
       "      <td>0.653946</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.818707</td>\n",
       "      <td>0.760476</td>\n",
       "      <td>0.795374</td>\n",
       "      <td>0.678708</td>\n",
       "      <td>0.689660</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.845374</td>\n",
       "      <td>0.639134</td>\n",
       "      <td>0.580513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.711905</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.760476</td>\n",
       "      <td>0.760476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.845374</td>\n",
       "      <td>0.845374</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.810476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">9</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">espanhola</th>\n",
       "      <th>0.10</th>\n",
       "      <td>0.254707</td>\n",
       "      <td>0.194452</td>\n",
       "      <td>0.121593</td>\n",
       "      <td>0.124572</td>\n",
       "      <td>0.314518</td>\n",
       "      <td>0.207384</td>\n",
       "      <td>0.213988</td>\n",
       "      <td>0.191278</td>\n",
       "      <td>0.244344</td>\n",
       "      <td>0.179848</td>\n",
       "      <td>0.174684</td>\n",
       "      <td>0.127803</td>\n",
       "      <td>0.282085</td>\n",
       "      <td>0.132638</td>\n",
       "      <td>0.170099</td>\n",
       "      <td>0.177719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.637381</td>\n",
       "      <td>0.601377</td>\n",
       "      <td>0.586754</td>\n",
       "      <td>0.565370</td>\n",
       "      <td>0.693387</td>\n",
       "      <td>0.677050</td>\n",
       "      <td>0.654218</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.572330</td>\n",
       "      <td>0.479638</td>\n",
       "      <td>0.377481</td>\n",
       "      <td>0.351155</td>\n",
       "      <td>0.622276</td>\n",
       "      <td>0.657619</td>\n",
       "      <td>0.537626</td>\n",
       "      <td>0.508009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.687806</td>\n",
       "      <td>0.710689</td>\n",
       "      <td>0.677959</td>\n",
       "      <td>0.682335</td>\n",
       "      <td>0.808265</td>\n",
       "      <td>0.810578</td>\n",
       "      <td>0.779728</td>\n",
       "      <td>0.764167</td>\n",
       "      <td>0.677789</td>\n",
       "      <td>0.613730</td>\n",
       "      <td>0.336094</td>\n",
       "      <td>0.366017</td>\n",
       "      <td>0.806803</td>\n",
       "      <td>0.767485</td>\n",
       "      <td>0.692840</td>\n",
       "      <td>0.680561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.799439</td>\n",
       "      <td>0.811003</td>\n",
       "      <td>0.787942</td>\n",
       "      <td>0.771037</td>\n",
       "      <td>0.857109</td>\n",
       "      <td>0.866922</td>\n",
       "      <td>0.851003</td>\n",
       "      <td>0.842279</td>\n",
       "      <td>0.730850</td>\n",
       "      <td>0.691066</td>\n",
       "      <td>0.452409</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.831939</td>\n",
       "      <td>0.813929</td>\n",
       "      <td>0.809711</td>\n",
       "      <td>0.791258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.802279</td>\n",
       "      <td>0.839456</td>\n",
       "      <td>0.821207</td>\n",
       "      <td>0.835969</td>\n",
       "      <td>0.869813</td>\n",
       "      <td>0.888469</td>\n",
       "      <td>0.879694</td>\n",
       "      <td>0.871122</td>\n",
       "      <td>0.771139</td>\n",
       "      <td>0.724388</td>\n",
       "      <td>0.543642</td>\n",
       "      <td>0.419274</td>\n",
       "      <td>0.863146</td>\n",
       "      <td>0.871633</td>\n",
       "      <td>0.853265</td>\n",
       "      <td>0.825646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.823146</td>\n",
       "      <td>0.848214</td>\n",
       "      <td>0.870765</td>\n",
       "      <td>0.847755</td>\n",
       "      <td>0.862466</td>\n",
       "      <td>0.881122</td>\n",
       "      <td>0.894252</td>\n",
       "      <td>0.885680</td>\n",
       "      <td>0.830765</td>\n",
       "      <td>0.823367</td>\n",
       "      <td>0.783622</td>\n",
       "      <td>0.740011</td>\n",
       "      <td>0.878742</td>\n",
       "      <td>0.888469</td>\n",
       "      <td>0.878792</td>\n",
       "      <td>0.878792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">10</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">espanhola</th>\n",
       "      <th>0.10</th>\n",
       "      <td>0.496295</td>\n",
       "      <td>0.430703</td>\n",
       "      <td>0.299048</td>\n",
       "      <td>0.390476</td>\n",
       "      <td>0.351941</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.280087</td>\n",
       "      <td>0.498844</td>\n",
       "      <td>0.359864</td>\n",
       "      <td>0.321769</td>\n",
       "      <td>0.387310</td>\n",
       "      <td>0.424762</td>\n",
       "      <td>0.337370</td>\n",
       "      <td>0.354444</td>\n",
       "      <td>0.269206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.739206</td>\n",
       "      <td>0.518367</td>\n",
       "      <td>0.618095</td>\n",
       "      <td>0.563352</td>\n",
       "      <td>0.631701</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.653810</td>\n",
       "      <td>0.556190</td>\n",
       "      <td>0.729479</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.416463</td>\n",
       "      <td>0.724082</td>\n",
       "      <td>0.490635</td>\n",
       "      <td>0.545547</td>\n",
       "      <td>0.514372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.811701</td>\n",
       "      <td>0.529388</td>\n",
       "      <td>0.572925</td>\n",
       "      <td>0.569116</td>\n",
       "      <td>0.833469</td>\n",
       "      <td>0.668658</td>\n",
       "      <td>0.569116</td>\n",
       "      <td>0.569116</td>\n",
       "      <td>0.741429</td>\n",
       "      <td>0.429116</td>\n",
       "      <td>0.458810</td>\n",
       "      <td>0.371825</td>\n",
       "      <td>0.806803</td>\n",
       "      <td>0.572925</td>\n",
       "      <td>0.569116</td>\n",
       "      <td>0.569116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.858503</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.814558</td>\n",
       "      <td>0.814558</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.825850</td>\n",
       "      <td>0.438367</td>\n",
       "      <td>0.512381</td>\n",
       "      <td>0.468844</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.814558</td>\n",
       "      <td>0.814558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.858503</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.858503</td>\n",
       "      <td>0.806803</td>\n",
       "      <td>0.509116</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.852517</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.881905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.912381</td>\n",
       "      <td>0.908571</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.908571</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.908571</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.939048</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.908571</td>\n",
       "      <td>0.881905</td>\n",
       "      <td>0.881905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "norm                                     l1                                \\\n",
       "sublinear_tf                          False                                 \n",
       "ngram_range                          (2, 3)    (2, 4)    (2, 5)    (3, 5)   \n",
       "problem language  PCA_componentes                                           \n",
       "1       inglesa   0.10             0.289050  0.213501  0.135221  0.144955   \n",
       "                  0.25             0.404768  0.475246  0.473339  0.432229   \n",
       "                  0.50             0.587794  0.633503  0.690323  0.670391   \n",
       "                  0.75             0.640255  0.734388  0.767228  0.759660   \n",
       "                  0.90             0.668231  0.783061  0.811412  0.795527   \n",
       "                  0.99             0.680153  0.786650  0.803554  0.803350   \n",
       "2       inglesa   0.10             0.484966  0.406753  0.359320  0.316871   \n",
       "                  0.25             0.760476  0.564762  0.518095  0.487619   \n",
       "                  0.50             0.885238  0.824286  0.802993  0.802993   \n",
       "                  0.75             0.883333  0.852857  0.882993  0.882993   \n",
       "                  0.90             0.883333  0.854762  0.854422  0.882993   \n",
       "                  0.99             0.883333  0.940136  0.940136  0.970612   \n",
       "3       francesa  0.10             0.270412  0.167608  0.158879  0.162775   \n",
       "                  0.25             0.471545  0.475370  0.456113  0.464135   \n",
       "                  0.50             0.708559  0.756378  0.691395  0.694847   \n",
       "                  0.75             0.772194  0.746259  0.738214  0.714065   \n",
       "                  0.90             0.769881  0.768180  0.783741  0.768792   \n",
       "                  0.99             0.800204  0.781599  0.772483  0.771531   \n",
       "4       francesa  0.10             0.269841  0.200779  0.232894  0.206111   \n",
       "                  0.25             0.357857  0.384005  0.455007  0.455007   \n",
       "                  0.50             0.648866  0.644762  0.630000  0.630000   \n",
       "                  0.75             0.739932  0.792993  0.854422  0.825034   \n",
       "                  0.90             0.775238  0.799524  0.796463  0.796463   \n",
       "                  0.99             0.775238  0.799524  0.825034  0.825034   \n",
       "5       italiana  0.10             0.251173  0.216178  0.145058  0.139421   \n",
       "                  0.25             0.418810  0.447969  0.409962  0.396136   \n",
       "                  0.50             0.628690  0.574171  0.529200  0.546047   \n",
       "                  0.75             0.659881  0.625617  0.623764  0.624443   \n",
       "                  0.90             0.660391  0.634120  0.632602  0.619031   \n",
       "                  0.99             0.653639  0.628810  0.642823  0.651173   \n",
       "6       italiana  0.10             0.344898  0.241088  0.232344  0.225541   \n",
       "                  0.25             0.657687  0.464444  0.425714  0.412857   \n",
       "                  0.50             0.910748  0.875850  0.880272  0.849796   \n",
       "                  0.75             0.940136  0.909660  0.911565  0.881088   \n",
       "                  0.90             0.880272  0.940136  0.940136  0.940136   \n",
       "                  0.99             0.940136  0.970612  0.940136  0.940136   \n",
       "7       polonesa  0.10             0.292150  0.220749  0.163698  0.178481   \n",
       "                  0.25             0.651990  0.565428  0.624476  0.639136   \n",
       "                  0.50             0.713302  0.748078  0.750459  0.739779   \n",
       "                  0.75             0.750425  0.760068  0.806412  0.791293   \n",
       "                  0.90             0.782041  0.773350  0.777874  0.770255   \n",
       "                  0.99             0.782296  0.792517  0.770255  0.770255   \n",
       "8       polonesa  0.10             0.153095  0.210037  0.143377  0.143377   \n",
       "                  0.25             0.482944  0.688435  0.470037  0.373095   \n",
       "                  0.50             0.564372  0.657229  0.567415  0.666191   \n",
       "                  0.75             0.711905  0.760476  0.632993  0.654422   \n",
       "                  0.90             0.711905  0.760476  0.678708  0.653946   \n",
       "                  0.99             0.711905  0.810476  0.810476  0.810476   \n",
       "9       espanhola 0.10             0.254707  0.194452  0.121593  0.124572   \n",
       "                  0.25             0.637381  0.601377  0.586754  0.565370   \n",
       "                  0.50             0.687806  0.710689  0.677959  0.682335   \n",
       "                  0.75             0.799439  0.811003  0.787942  0.771037   \n",
       "                  0.90             0.802279  0.839456  0.821207  0.835969   \n",
       "                  0.99             0.823146  0.848214  0.870765  0.847755   \n",
       "10      espanhola 0.10             0.496295  0.430703  0.299048  0.390476   \n",
       "                  0.25             0.739206  0.518367  0.618095  0.563352   \n",
       "                  0.50             0.811701  0.529388  0.572925  0.569116   \n",
       "                  0.75             0.858503  0.793333  0.814558  0.814558   \n",
       "                  0.90             0.858503  0.852517  0.881905  0.881905   \n",
       "                  0.99             0.912381  0.908571  0.881905  0.881905   \n",
       "\n",
       "norm                                                                       \\\n",
       "sublinear_tf                          True                                  \n",
       "ngram_range                          (2, 3)    (2, 4)    (2, 5)    (3, 5)   \n",
       "problem language  PCA_componentes                                           \n",
       "1       inglesa   0.10             0.247157  0.202389  0.111574  0.105737   \n",
       "                  0.25             0.486210  0.566849  0.525181  0.573929   \n",
       "                  0.50             0.709813  0.784048  0.788146  0.789694   \n",
       "                  0.75             0.742721  0.792092  0.790527  0.792568   \n",
       "                  0.90             0.772653  0.799745  0.813895  0.806548   \n",
       "                  0.99             0.791224  0.826905  0.833299  0.826718   \n",
       "2       inglesa   0.10             0.380245  0.357388  0.178926  0.177547   \n",
       "                  0.25             0.707143  0.648095  0.681905  0.611787   \n",
       "                  0.50             0.885238  0.831837  0.852517  0.852517   \n",
       "                  0.75             0.909660  0.970612  0.940136  0.940136   \n",
       "                  0.90             0.883333  0.970612  0.940136  0.940136   \n",
       "                  0.99             0.883333  0.970612  0.970612  0.970612   \n",
       "3       francesa  0.10             0.204083  0.170256  0.114198  0.122971   \n",
       "                  0.25             0.545282  0.606876  0.584609  0.559626   \n",
       "                  0.50             0.746003  0.771156  0.787160  0.767602   \n",
       "                  0.75             0.740901  0.778878  0.767908  0.759388   \n",
       "                  0.90             0.778759  0.779949  0.756105  0.762296   \n",
       "                  0.99             0.777942  0.788078  0.762500  0.775272   \n",
       "4       francesa  0.10             0.228571  0.162313  0.182004  0.168911   \n",
       "                  0.25             0.321148  0.394353  0.557415  0.557415   \n",
       "                  0.50             0.743401  0.789184  0.792993  0.792993   \n",
       "                  0.75             0.735170  0.825850  0.825034  0.825034   \n",
       "                  0.90             0.743810  0.825850  0.825034  0.825034   \n",
       "                  0.99             0.743810  0.799524  0.854422  0.854422   \n",
       "5       italiana  0.10             0.244575  0.197154  0.101712  0.102508   \n",
       "                  0.25             0.444544  0.376460  0.383626  0.390683   \n",
       "                  0.50             0.625017  0.629626  0.639205  0.605091   \n",
       "                  0.75             0.688793  0.657840  0.662126  0.644346   \n",
       "                  0.90             0.690102  0.672058  0.672126  0.624558   \n",
       "                  0.99             0.680578  0.664898  0.676735  0.665816   \n",
       "6       italiana  0.10             0.332032  0.188571  0.117698  0.145173   \n",
       "                  0.25             0.810563  0.656599  0.523129  0.523129   \n",
       "                  0.50             0.910748  0.886258  0.855782  0.886258   \n",
       "                  0.75             0.941225  0.940136  0.909660  0.909660   \n",
       "                  0.90             0.941225  0.970612  0.940136  0.940136   \n",
       "                  0.99             0.910748  0.940136  0.909660  0.909660   \n",
       "7       polonesa  0.10             0.354521  0.199089  0.183448  0.178918   \n",
       "                  0.25             0.695111  0.704972  0.722607  0.698151   \n",
       "                  0.50             0.792721  0.794218  0.764881  0.752568   \n",
       "                  0.75             0.804864  0.799745  0.815170  0.798826   \n",
       "                  0.90             0.786242  0.815289  0.792755  0.784031   \n",
       "                  0.99             0.784065  0.816241  0.814065  0.798826   \n",
       "8       polonesa  0.10             0.154113  0.120513  0.143377  0.183203   \n",
       "                  0.25             0.511429  0.542993  0.647075  0.622857   \n",
       "                  0.50             0.710952  0.657229  0.506753  0.506753   \n",
       "                  0.75             0.760476  0.810476  0.733810  0.607143   \n",
       "                  0.90             0.810476  0.810476  0.810476  0.818707   \n",
       "                  0.99             0.810476  0.810476  0.810476  0.760476   \n",
       "9       espanhola 0.10             0.314518  0.207384  0.213988  0.191278   \n",
       "                  0.25             0.693387  0.677050  0.654218  0.647619   \n",
       "                  0.50             0.808265  0.810578  0.779728  0.764167   \n",
       "                  0.75             0.857109  0.866922  0.851003  0.842279   \n",
       "                  0.90             0.869813  0.888469  0.879694  0.871122   \n",
       "                  0.99             0.862466  0.881122  0.894252  0.885680   \n",
       "10      espanhola 0.10             0.351941  0.280000  0.283333  0.280087   \n",
       "                  0.25             0.631701  0.509615  0.653810  0.556190   \n",
       "                  0.50             0.833469  0.668658  0.569116  0.569116   \n",
       "                  0.75             0.852517  0.852517  0.852517  0.852517   \n",
       "                  0.90             0.852517  0.881905  0.881905  0.881905   \n",
       "                  0.99             0.881905  0.908571  0.881905  0.881905   \n",
       "\n",
       "norm                                     l2                                \\\n",
       "sublinear_tf                          False                                 \n",
       "ngram_range                          (2, 3)    (2, 4)    (2, 5)    (3, 5)   \n",
       "problem language  PCA_componentes                                           \n",
       "1       inglesa   0.10             0.261712  0.147346  0.157838  0.142236   \n",
       "                  0.25             0.356450  0.344444  0.244823  0.245465   \n",
       "                  0.50             0.530153  0.415823  0.346530  0.319832   \n",
       "                  0.75             0.591655  0.600161  0.603951  0.493628   \n",
       "                  0.90             0.650258  0.718389  0.614846  0.588564   \n",
       "                  0.99             0.669558  0.759524  0.714898  0.734116   \n",
       "2       inglesa   0.10             0.357619  0.321465  0.171190  0.207446   \n",
       "                  0.25             0.635374  0.438730  0.428540  0.339048   \n",
       "                  0.50             0.854762  0.738231  0.643537  0.553878   \n",
       "                  0.75             0.795714  0.784422  0.747279  0.492064   \n",
       "                  0.90             0.880272  0.802993  0.721497  0.593469   \n",
       "                  0.99             0.883333  0.879184  0.940136  0.940136   \n",
       "3       francesa  0.10             0.238987  0.204862  0.142976  0.143707   \n",
       "                  0.25             0.431922  0.405226  0.300296  0.318425   \n",
       "                  0.50             0.656701  0.553418  0.441724  0.399718   \n",
       "                  0.75             0.722483  0.668066  0.411551  0.430339   \n",
       "                  0.90             0.762194  0.695867  0.427517  0.495781   \n",
       "                  0.99             0.794184  0.760782  0.732101  0.723802   \n",
       "4       francesa  0.10             0.210159  0.243307  0.232072  0.214062   \n",
       "                  0.25             0.348398  0.485065  0.501039  0.432381   \n",
       "                  0.50             0.574286  0.564286  0.484848  0.578182   \n",
       "                  0.75             0.768503  0.804898  0.704558  0.654558   \n",
       "                  0.90             0.743810  0.778571  0.775510  0.807415   \n",
       "                  0.99             0.743810  0.799524  0.853605  0.853605   \n",
       "5       italiana  0.10             0.286071  0.177957  0.131691  0.124456   \n",
       "                  0.25             0.439610  0.401461  0.332051  0.391267   \n",
       "                  0.50             0.630935  0.488163  0.402989  0.403326   \n",
       "                  0.75             0.593423  0.534399  0.386599  0.420805   \n",
       "                  0.90             0.622835  0.541967  0.404153  0.425676   \n",
       "                  0.99             0.628469  0.550351  0.392630  0.409102   \n",
       "6       italiana  0.10             0.529765  0.281667  0.126176  0.126176   \n",
       "                  0.25             0.836463  0.464725  0.280224  0.494921   \n",
       "                  0.50             0.880272  0.787891  0.698980  0.804898   \n",
       "                  0.75             0.910748  0.910748  0.909660  0.780476   \n",
       "                  0.90             0.880272  0.909660  0.833469  0.881088   \n",
       "                  0.99             0.880272  0.970612  0.940136  0.940136   \n",
       "7       polonesa  0.10             0.305014  0.278441  0.137852  0.177517   \n",
       "                  0.25             0.581215  0.408031  0.224665  0.252983   \n",
       "                  0.50             0.728316  0.611486  0.447133  0.424007   \n",
       "                  0.75             0.722913  0.673254  0.438658  0.446607   \n",
       "                  0.90             0.750561  0.652215  0.528016  0.438444   \n",
       "                  0.99             0.748078  0.723106  0.712921  0.707415   \n",
       "8       polonesa  0.10             0.130711  0.099048  0.159878  0.223946   \n",
       "                  0.25             0.321224  0.433968  0.202177  0.204538   \n",
       "                  0.50             0.628571  0.451429  0.201000  0.405102   \n",
       "                  0.75             0.711905  0.690272  0.693469  0.604985   \n",
       "                  0.90             0.760476  0.795374  0.678708  0.689660   \n",
       "                  0.99             0.760476  0.810476  0.845374  0.845374   \n",
       "9       espanhola 0.10             0.244344  0.179848  0.174684  0.127803   \n",
       "                  0.25             0.572330  0.479638  0.377481  0.351155   \n",
       "                  0.50             0.677789  0.613730  0.336094  0.366017   \n",
       "                  0.75             0.730850  0.691066  0.452409  0.331250   \n",
       "                  0.90             0.771139  0.724388  0.543642  0.419274   \n",
       "                  0.99             0.830765  0.823367  0.783622  0.740011   \n",
       "10      espanhola 0.10             0.498844  0.359864  0.321769  0.387310   \n",
       "                  0.25             0.729479  0.671429  0.411111  0.416463   \n",
       "                  0.50             0.741429  0.429116  0.458810  0.371825   \n",
       "                  0.75             0.825850  0.438367  0.512381  0.468844   \n",
       "                  0.90             0.858503  0.806803  0.509116  0.377778   \n",
       "                  0.99             0.881905  0.908571  0.881905  0.939048   \n",
       "\n",
       "norm                                                                       \n",
       "sublinear_tf                          True                                 \n",
       "ngram_range                          (2, 3)    (2, 4)    (2, 5)    (3, 5)  \n",
       "problem language  PCA_componentes                                          \n",
       "1       inglesa   0.10             0.225762  0.201633  0.175090  0.155582  \n",
       "                  0.25             0.469694  0.452204  0.433352  0.397762  \n",
       "                  0.50             0.697823  0.729201  0.706701  0.723929  \n",
       "                  0.75             0.735527  0.773520  0.781173  0.747840  \n",
       "                  0.90             0.794252  0.817925  0.792619  0.794422  \n",
       "                  0.99             0.816037  0.817925  0.818844  0.826463  \n",
       "2       inglesa   0.10             0.380245  0.321991  0.275873  0.253968  \n",
       "                  0.25             0.722313  0.649932  0.491198  0.445121  \n",
       "                  0.50             0.824286  0.879184  0.810884  0.839456  \n",
       "                  0.75             0.909660  0.909660  0.845034  0.882993  \n",
       "                  0.90             0.909660  0.940136  0.882993  0.882993  \n",
       "                  0.99             0.909660  0.970612  0.970612  0.970612  \n",
       "3       francesa  0.10             0.209215  0.205600  0.170882  0.168999  \n",
       "                  0.25             0.535779  0.531665  0.480772  0.475074  \n",
       "                  0.50             0.744269  0.741054  0.757160  0.723639  \n",
       "                  0.75             0.747670  0.758878  0.754218  0.759932  \n",
       "                  0.90             0.783929  0.774983  0.755034  0.752262  \n",
       "                  0.99             0.786310  0.768537  0.763452  0.776020  \n",
       "4       francesa  0.10             0.221905  0.182313  0.201884  0.201939  \n",
       "                  0.25             0.377056  0.318961  0.411111  0.428341  \n",
       "                  0.50             0.711020  0.703333  0.582540  0.604762  \n",
       "                  0.75             0.799524  0.854422  0.818707  0.780748  \n",
       "                  0.90             0.772381  0.854422  0.825034  0.790136  \n",
       "                  0.99             0.799048  0.828095  0.854422  0.854422  \n",
       "5       italiana  0.10             0.241795  0.193724  0.115280  0.139841  \n",
       "                  0.25             0.451446  0.490510  0.482947  0.469167  \n",
       "                  0.50             0.630374  0.644426  0.606897  0.588666  \n",
       "                  0.75             0.701003  0.665425  0.670192  0.643741  \n",
       "                  0.90             0.675459  0.691514  0.686003  0.699167  \n",
       "                  0.99             0.681854  0.688469  0.658775  0.660000  \n",
       "6       italiana  0.10             0.297936  0.172511  0.133095  0.181349  \n",
       "                  0.25             0.707706  0.684422  0.632925  0.676463  \n",
       "                  0.50             0.941225  0.851701  0.881088  0.881088  \n",
       "                  0.75             0.910748  0.909660  0.909660  0.909660  \n",
       "                  0.90             0.910748  0.970612  0.940136  0.909660  \n",
       "                  0.99             0.910748  0.970612  0.940136  0.940136  \n",
       "7       polonesa  0.10             0.319800  0.196897  0.154359  0.183950  \n",
       "                  0.25             0.676658  0.704995  0.599972  0.563194  \n",
       "                  0.50             0.790816  0.788759  0.728027  0.706429  \n",
       "                  0.75             0.778469  0.779694  0.800136  0.793044  \n",
       "                  0.90             0.764643  0.812483  0.791599  0.800323  \n",
       "                  0.99             0.784065  0.809507  0.814065  0.799099  \n",
       "8       polonesa  0.10             0.118730  0.104524  0.143377  0.190043  \n",
       "                  0.25             0.462177  0.654762  0.435952  0.344220  \n",
       "                  0.50             0.657229  0.617415  0.667619  0.692789  \n",
       "                  0.75             0.810476  0.810476  0.631088  0.631088  \n",
       "                  0.90             0.810476  0.845374  0.639134  0.580513  \n",
       "                  0.99             0.810476  0.810476  0.810476  0.810476  \n",
       "9       espanhola 0.10             0.282085  0.132638  0.170099  0.177719  \n",
       "                  0.25             0.622276  0.657619  0.537626  0.508009  \n",
       "                  0.50             0.806803  0.767485  0.692840  0.680561  \n",
       "                  0.75             0.831939  0.813929  0.809711  0.791258  \n",
       "                  0.90             0.863146  0.871633  0.853265  0.825646  \n",
       "                  0.99             0.878742  0.888469  0.878792  0.878792  \n",
       "10      espanhola 0.10             0.424762  0.337370  0.354444  0.269206  \n",
       "                  0.25             0.724082  0.490635  0.545547  0.514372  \n",
       "                  0.50             0.806803  0.572925  0.569116  0.569116  \n",
       "                  0.75             0.852517  0.852517  0.814558  0.814558  \n",
       "                  0.90             0.852517  0.852517  0.881905  0.881905  \n",
       "                  0.99             0.881905  0.908571  0.881905  0.881905  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCV.pivot_table(\n",
    "            index=['problem','language','PCA_componentes'],\n",
    "            columns=['norm','sublinear_tf', 'ngram_range'],\n",
    "            values='mean_test_score'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Medida F1 para os parâmetros }\n",
      "\\begin{tabular}{llllrrrr}\n",
      "\\toprule\n",
      " & & & ngram\\_range & (2, 3) & (2, 4) & (2, 5) & (3, 5) \\\\\n",
      "problem & language & sublinear\\_tf & norm & & & & \\\\\n",
      "\\midrule\n",
      "1 & inglesa & False & l1 & 0.680 & 0.787 & 0.804 & 0.803 \\\\\n",
      " & & & l2 & 0.670 & 0.760 & 0.715 & 0.734 \\\\\n",
      " & & True & l1 & 0.791 & 0.827 & 0.833 & 0.827 \\\\\n",
      " & & & l2 & 0.816 & 0.818 & 0.819 & 0.826 \\\\\n",
      "2 & inglesa & False & l1 & 0.883 & 0.940 & 0.940 & 0.971 \\\\\n",
      " & & & l2 & 0.883 & 0.879 & 0.940 & 0.940 \\\\\n",
      " & & True & l1 & 0.883 & 0.971 & 0.971 & 0.971 \\\\\n",
      " & & & l2 & 0.910 & 0.971 & 0.971 & 0.971 \\\\\n",
      "3 & francesa & False & l1 & 0.800 & 0.782 & 0.772 & 0.772 \\\\\n",
      " & & & l2 & 0.794 & 0.761 & 0.732 & 0.724 \\\\\n",
      " & & True & l1 & 0.778 & 0.788 & 0.762 & 0.775 \\\\\n",
      " & & & l2 & 0.786 & 0.769 & 0.763 & 0.776 \\\\\n",
      "4 & francesa & False & l1 & 0.775 & 0.800 & 0.825 & 0.825 \\\\\n",
      " & & & l2 & 0.744 & 0.800 & 0.854 & 0.854 \\\\\n",
      " & & True & l1 & 0.744 & 0.800 & 0.854 & 0.854 \\\\\n",
      " & & & l2 & 0.799 & 0.828 & 0.854 & 0.854 \\\\\n",
      "5 & italiana & False & l1 & 0.654 & 0.629 & 0.643 & 0.651 \\\\\n",
      " & & & l2 & 0.628 & 0.550 & 0.393 & 0.409 \\\\\n",
      " & & True & l1 & 0.681 & 0.665 & 0.677 & 0.666 \\\\\n",
      " & & & l2 & 0.682 & 0.688 & 0.659 & 0.660 \\\\\n",
      "6 & italiana & False & l1 & 0.940 & 0.971 & 0.940 & 0.940 \\\\\n",
      " & & & l2 & 0.880 & 0.971 & 0.940 & 0.940 \\\\\n",
      " & & True & l1 & 0.911 & 0.940 & 0.910 & 0.910 \\\\\n",
      " & & & l2 & 0.911 & 0.971 & 0.940 & 0.940 \\\\\n",
      "7 & polonesa & False & l1 & 0.782 & 0.793 & 0.770 & 0.770 \\\\\n",
      " & & & l2 & 0.748 & 0.723 & 0.713 & 0.707 \\\\\n",
      " & & True & l1 & 0.784 & 0.816 & 0.814 & 0.799 \\\\\n",
      " & & & l2 & 0.784 & 0.810 & 0.814 & 0.799 \\\\\n",
      "8 & polonesa & False & l1 & 0.712 & 0.810 & 0.810 & 0.810 \\\\\n",
      " & & & l2 & 0.760 & 0.810 & 0.845 & 0.845 \\\\\n",
      " & & True & l1 & 0.810 & 0.810 & 0.810 & 0.760 \\\\\n",
      " & & & l2 & 0.810 & 0.810 & 0.810 & 0.810 \\\\\n",
      "9 & espanhola & False & l1 & 0.823 & 0.848 & 0.871 & 0.848 \\\\\n",
      " & & & l2 & 0.831 & 0.823 & 0.784 & 0.740 \\\\\n",
      " & & True & l1 & 0.862 & 0.881 & 0.894 & 0.886 \\\\\n",
      " & & & l2 & 0.879 & 0.888 & 0.879 & 0.879 \\\\\n",
      "10 & espanhola & False & l1 & 0.912 & 0.909 & 0.882 & 0.882 \\\\\n",
      " & & & l2 & 0.882 & 0.909 & 0.882 & 0.939 \\\\\n",
      " & & True & l1 & 0.882 & 0.909 & 0.882 & 0.882 \\\\\n",
      " & & & l2 & 0.882 & 0.909 & 0.882 & 0.882 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\label{tab:modelocaracter}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.precision = 3  \n",
    "print(u\"\\\\begin{table}[h]\\n\\\\centering\\n\\\\caption{Medida F1 para os parâmetros }\")\n",
    "\n",
    "print(re.sub(r'[ ]{2,}',' ',dfCV[dfCV.PCA_componentes >= 0.99].pivot_table(\n",
    "        index=['problem','language','sublinear_tf','norm'],\n",
    "        columns=['ngram_range'],\n",
    "        values='mean_test_score'\n",
    "    ).to_latex()))\n",
    "print (\"\\label{tab:modelocaracter}\")\n",
    "print(r\"\\end{table}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
